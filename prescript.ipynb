{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MACROTABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INDEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pd.read_csv('data/index.zip') #reading index data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Where locality is missing subregion2 is not and viceversa --> cannot assure that these 2 are equal.\n",
    "#Drop both locality and subregion as these are not adding anything to country level analysis (and we already have subregion1).\n",
    "index = index.drop(columns = ['locality_code', 'locality_name']) \n",
    "index = index.drop(columns = ['subregion2_code', 'subregion2_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These are unique value keys, so there is no way to impute them\n",
    "#They are not related at all between each other (none is derived from the other)\n",
    "#Do not provide any value for country, so they can be dropped\n",
    "index = index.drop(columns = ['place_id', 'wikidata_id', 'datacommons_id']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The columns iso_3166_1_alpha_2, iso_3166_1_alpha_3 and aggregation_level are not provivding any value to our analysis\n",
    "## So we drop them\n",
    "index = index.drop(columns = ['iso_3166_1_alpha_2', 'iso_3166_1_alpha_3', 'aggregation_level'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HEALTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "health = pd.read_csv('data/health.zip') #1. Reading health data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "health = health.dropna(axis = 'columns', how = 'all') #2. dropping all empty columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VACCINATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaccinations = pd.read_csv('data/vaccinations.zip') #1. Reading vaccinations data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaccinations = vaccinations.dropna(axis = 'columns', how = 'all') #2. dropping all empty columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaccinations = vaccinations.fillna(0) \n",
    "# All missing values in the new_persons_fully_vaccinated column are for the first day (or the day previous to the first) of vaccinations \n",
    "## within the time perod of the dataset. \n",
    "# Therefore these must be 0, hence we impute these missing values with 0s as such."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaccinations['date'] = pd.to_datetime(vaccinations['date'], format=\"%Y-%m-%d\") #4. Changing date to datetime format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforming vaccinations to be able to merge it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the column date from day to week:\n",
    "vaccinations['date'] =  vaccinations['date'].dt.to_period(\"W\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaccinations= vaccinations.groupby(['date','location_key'])[['new_persons_fully_vaccinated', 'cumulative_persons_fully_vaccinated']].sum()\n",
    "# Grouping by date (week) and location_key, as these are the indices we want.\n",
    "#We use the sum metric for the group by."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formatting the index\n",
    "vaccinations = vaccinations.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEMOGRAPHICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographics = pd.read_csv('data/demographics.zip') #1. Reading demographics data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographics = demographics.dropna(axis = 'columns', how = 'all') #2. dropping all empty columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We also drop the population density column, as it has over 90% missing values,\n",
    "## and given that this column is not really aggregatable at a country level \n",
    "### (it is not aggregatable as it is a ratio and not an absolute population value, unlike the other population columns).\n",
    "demographics = demographics.drop(columns = ['population_density'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The rest of columns with null values we will clean once we have merged all the datasets. \n",
    "# This is because, these are absolute population variables that can be aggregated at a country level. \n",
    "# In other words, in doesn't matter if we don't clean them now, because once we group by country name in the macrotable (using the sum as a metric), \n",
    "## the null values will disappear (as they are not taken into account in the grouping). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HOSPITALIZATIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through analysis of the hospitalizations dataset, we discovered there were only 7 location keys (all corresponding to locations within the United States), whereas in other tables (such as index) there were 5121, for all (4) countries.\n",
    "\n",
    "If we merge the hospitalizations dataset, this would cause there to be many null values (for all countries except the US). This is not feasible as per the assignment instructions (that the macrotable cannot have any null/missing values). A solution to this issue would be to remove those null values in the macrotable, however, in that case we would have a macrotable with very few rows (and with values only for the US).\n",
    "\n",
    "Even though we recognize the value of the hospitalization data:\n",
    " 1. It is not essential to predict deaths (which is our final objective)\n",
    " 2. We prioritize a large dataset covering all countries over having a small macrotable covering data for the US only.\n",
    "\n",
    " In conclusion, we decied not merge the hospitalizations table to our macrotable, and drop it from our analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EPIDEMIOLOGY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "epidemiology = pd.read_csv('data/epidemiology.zip') #1. Reading epidemiology data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "epidemiology = epidemiology.drop(columns = ['new_tested', 'cumulative_tested'])\n",
    "#2. Drop new_tested and cumulative_tested as both these columns are nearly completely null\n",
    "# Also, there is no way to calculate/impute tested as we would need another column with the confirmed negative cases of covid (which we don't)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As recovered columns are over 90% null, we drop these columns.\n",
    "epidemiology = epidemiology.drop(columns = ['new_recovered', 'cumulative_recovered'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing nulls in new_confirmed by calculating its value from two consecutive cumulative_confirmed values \n",
    "for i in range(len(epidemiology)):\n",
    "    if pd.isna(epidemiology.at[i, \"new_confirmed\"]) == True and epidemiology.at[i, \"location_key\"] == epidemiology.at[(i-1), \"location_key\"]:\n",
    "        epidemiology.at[i, \"new_confirmed\"] = epidemiology.at[i, \"cumulative_confirmed\"] - epidemiology.at[i-1, \"cumulative_confirmed\"]\n",
    "# Group by 'location_key' and find the indices of the rows with the minimum 'date' for each group\n",
    "min_date_indices = epidemiology.groupby('location_key')['date'].idxmin()\n",
    "\n",
    "# Subset the original DataFrame for rows with the minimum date for each location\n",
    "min_date_rows = epidemiology.loc[min_date_indices].copy()\n",
    "\n",
    "# Replace null values of 'new_confirmed' with 'cumulative_confirmed' for these rows\n",
    "min_date_rows.loc[min_date_rows['new_confirmed'].isnull(), 'new_confirmed'] = min_date_rows['cumulative_confirmed']\n",
    "\n",
    "# Update the original DataFrame with the modified rows for minimum dates\n",
    "epidemiology.update(min_date_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 1: Sort the dataframe by location_key and date\n",
    "epidemiology = epidemiology.sort_values(by=['location_key', 'date']).reset_index(drop=True)\n",
    "\n",
    "# Step 2: Forward-fill 'cumulative_deceased', but leave all-null groups as NaN\n",
    "epidemiology['cumulative_deceased'] = epidemiology.groupby('location_key')['cumulative_deceased'].transform(\n",
    "    lambda group: group.ffill() if group.notna().any() else group\n",
    ")\n",
    "\n",
    "# Step 3: Calculate 'new_deceased', leaving all-null groups as NaN\n",
    "epidemiology['new_deceased'] = epidemiology.groupby('location_key')['cumulative_deceased'].transform(\n",
    "    lambda group: group.diff() if group.notna().any() else group\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Afonso Santos\\AppData\\Local\\Temp\\ipykernel_5596\\32792377.py:11: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  region_date_avg = epidemiology.groupby(['region', 'date']).apply(\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# Step 4: Clip negative values to ensure 'new_deceased' and 'new_confirmed' are non-negative\n",
    "epidemiology['new_deceased'] = epidemiology['new_deceased'].clip(lower=0)\n",
    "epidemiology['new_confirmed'] = epidemiology['new_confirmed'].clip(lower=0)\n",
    "\n",
    "# Step 5: Extract region code (characters 4 and 5 from location_key)\n",
    "epidemiology['region'] = epidemiology['location_key'].str[3:5]\n",
    "\n",
    "# Step 6: Calculate the ratio 'x' (avg of new_deceased / avg of new_confirmed) per region and date\n",
    "region_date_avg = epidemiology.groupby(['region', 'date']).apply(\n",
    "    lambda group: group['new_deceased'].sum() / group['new_confirmed'].sum()\n",
    "    if group['new_confirmed'].sum() > 0 else 0\n",
    ").rename('x').reset_index()\n",
    "\n",
    "# Step 7: Merge 'x' back into the original dataframe\n",
    "epidemiology = epidemiology.merge(region_date_avg, on=['region', 'date'], how='left')\n",
    "\n",
    "# Step 8: Impute missing 'new_deceased' values using 'x * new_confirmed' and round to nearest integer if decimal part>0.05 for the death ratio to be close to germany's ratio\n",
    "\n",
    "epidemiology['new_deceased'] = epidemiology.apply(\n",
    "    lambda row: (\n",
    "        math.ceil(row['x'] * row['new_confirmed']) \n",
    "        if (row['x'] * row['new_confirmed']) % 1 > 0.05 \n",
    "        else round(row['x'] * row['new_confirmed'])\n",
    "    ) if pd.isna(row['new_deceased']) else row['new_deceased'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Step 9: Drop temporary columns if necessary\n",
    "epidemiology.drop(columns=['region', 'x'], inplace=True)\n",
    "f = epidemiology.sort_values(by=['location_key', 'date']).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10: Recalculate cumulative_deceased using new_deceased\n",
    "epidemiology['cumulative_deceased'] = epidemiology.groupby('location_key')['new_deceased'].transform(lambda group: group.cumsum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We change the datatype of 'date' from object to datetime64[ns] (i.e, the correct type)\n",
    "epidemiology['date'] = pd.to_datetime(epidemiology['date'], format=\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforming epidemiology to be able to merge it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the column date from day to week:\n",
    "epidemiology['date'] =  epidemiology['date'].dt.to_period(\"W\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "epidemiology= epidemiology.groupby(['date','location_key'])[['new_confirmed', 'new_deceased', 'cumulative_confirmed', 'cumulative_deceased']].sum()\n",
    "#Grouping by date (week) and location_key, as these are the indices we want.\n",
    "#We use the sum metric for the group by."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Formatting the index\n",
    "epidemiology = epidemiology.reset_index() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MERGING TABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged1 = pd.merge(index, demographics , on=[\"location_key\"], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_static = pd.merge(merged1, health, on=[\"location_key\"], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_intermediate_static = intermediate_static.groupby('country_code').agg({             \n",
    "'population': \"sum\",                                 \n",
    "'population_male': \"sum\",                         \n",
    "'population_female': \"sum\",                       \n",
    "'population_age_00_09': \"sum\",                   \n",
    "'population_age_10_19': \"sum\",                    \n",
    "'population_age_20_29': \"sum\",                     \n",
    "'population_age_30_39': \"sum\",                     \n",
    "'population_age_40_49': \"sum\",                     \n",
    "'population_age_50_59': \"sum\",                     \n",
    "'population_age_60_69': \"sum\",                     \n",
    "'population_age_70_79': \"sum\",                     \n",
    "'population_age_80_and_older': \"sum\",           \n",
    "'life_expectancy': \"mean\", \n",
    "}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_dynamic =  pd.merge(epidemiology, vaccinations, on=['date', \"location_key\"], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_dynamic['country_code'] = intermediate_dynamic['location_key'].str[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_intermediate_dynamic = intermediate_dynamic.groupby(['country_code', 'date']).sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "macrotable =  pd.merge(grouped_intermediate_dynamic, grouped_intermediate_static, on= [\"country_code\"], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We drop the following columns as these are not providing any value at the country level, and will make things more complicated when grouping\n",
    "macrotable = macrotable.drop(columns = ['location_key', 'cumulative_confirmed', 'cumulative_deceased', 'cumulative_persons_fully_vaccinated'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "macrotable['country_code'] = macrotable['country_code'].replace({\n",
    "    'DE': 'Germany',\n",
    "    'US': 'United States',\n",
    "    'ES': 'Spain',\n",
    "    'IT': 'Italy'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "macrotable = macrotable.rename(columns={'country_code': 'country_name'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_order = ['date',\n",
    "            'country_name', \n",
    "            'population',\n",
    "            'population_male',\t\n",
    "            'population_female',\n",
    "            'population_age_00_09',\t\n",
    "            'population_age_10_19',\t\n",
    "             'population_age_20_29',\t\n",
    "            'population_age_30_39',\t\n",
    "             'population_age_40_49',\t\n",
    "             'population_age_50_59',\t\n",
    "             'population_age_60_69',\t\n",
    "            'population_age_70_79',\t\n",
    "            'population_age_80_and_older',\t\n",
    "            'life_expectancy',\n",
    "             'new_confirmed',\t\n",
    "             'new_deceased',\t\n",
    "            'new_persons_fully_vaccinated']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "macrotable = macrotable[new_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "macrotable = macrotable.rename(columns={'date': 'week'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "group10_macrotable = macrotable.to_csv('group10_macrotable.csv', header=True, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
